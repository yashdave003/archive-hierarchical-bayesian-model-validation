{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from scipy import integrate, interpolate  \n",
    "from scipy.stats import gengamma, laplace, kstwo, ks_1samp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), \"testing-framework\"))\n",
    "\n",
    "def compute_prior_pdf(r, eta, n_samples = 10000, tail_bound = 0.05, n_tail = 5, scale = 1):\n",
    "\n",
    "    '''\n",
    "    Returns support and pdf for prior distribution\n",
    "    r : shape parameter controlling rate of exponentional decay\n",
    "    eta : controls roundedness of peak, and hence sparsity\n",
    "    scale : scale parameter\n",
    "    n_samples : number of points used to numerically approximate CDF\n",
    "    tail_bound : Uses Chebyshev's Inequality to bound the region of the CDF that is outside the coverage of xs\n",
    "    n_tail : Sets the number of points tha lie outside the coverage of xs to approximate tails if need be\n",
    "\n",
    "    Usage:\n",
    "    new_pdf = compute_prior_cdf(r = 0.1, eta = 0.001)\n",
    "    new_pdf(0.5343) returns CDF\n",
    "    Can also accept arrays\n",
    "    '''\n",
    "    \n",
    "    beta = (eta + 1.5)/r \n",
    "    var_prior = scale * scipy.special.gamma((eta + 1.5 + 2)/r)/scipy.special.gamma(beta)\n",
    "    cheby = np.sqrt(np.round(var_prior/(tail_bound)))\n",
    "    x_max = min(99, cheby) # introduced additional bound in case chebyshev is unwieldy\n",
    "    if cheby <= 99:\n",
    "        n_tail = 0\n",
    "        print(\"No Tail\")\n",
    "        \n",
    "    xs = np.linspace(-x_max, x_max, n_samples-2*n_tail)\n",
    "    xs = np.append(-np.logspace(np.log10(cheby), 2, n_tail), xs)\n",
    "    xs = np.append(xs, np.logspace(2, np.log10(cheby), n_tail))\n",
    "    prior_pdf = np.full(xs.shape, np.nan)\n",
    "\n",
    "    # Loop over xs\n",
    "    for j, x in enumerate(xs):\n",
    "\n",
    "        # Define integrands\n",
    "        def gauss_density(theta):\n",
    "            return (1./(np.sqrt(2*np.pi)*theta)) * np.exp(-0.5*(x/theta)**2)\n",
    "\n",
    "        def gen_gamma_density(theta):\n",
    "            return (r/scipy.special.gamma(beta)) * (1/scale) * (theta/scale)**(r*beta - 1) * np.exp(-(theta/scale)**r)\n",
    "\n",
    "        def integrand(theta):\n",
    "            return gauss_density(theta) * gen_gamma_density(theta)\n",
    "\n",
    "        # Integrate \n",
    "        prior_pdf[j] = integrate.quad(integrand, 0, np.inf)[0]\n",
    "    return xs, prior_pdf\n",
    "\n",
    "def compute_prior_cdf(r, eta, n_samples = 1000, tail_bound = 0.05, n_tail = 5, scale = 1):\n",
    "\n",
    "    '''\n",
    "    Returns PPoly-type function that approximates the prior CDF of the signal x\n",
    "    r : shape parameter controlling rate of exponentional decay\n",
    "    eta : controls roundedness of peak, and hence sparsity\n",
    "    scale : scale parameter\n",
    "    n_samples : number of points used to numerically approximate CDF\n",
    "    tail_bound : Uses Chebyshev's Inequality to bound the region of the CDF that is outside the coverage of xs\n",
    "    n_tail : Sets the number of points tha lie outside the coverage of xs to approximate tails if need be\n",
    "\n",
    "    Usage:\n",
    "    new_cdf = compute_prior_cdf(r = 0.1, eta = 0.001)\n",
    "    new_cdf(0.5343) returns CDF\n",
    "    Can also accept arrays\n",
    "    '''\n",
    "    \n",
    "    beta = (eta + 1.5)/r \n",
    "    var_prior = scale * scipy.special.gamma((eta + 1.5 + 2)/r)/scipy.special.gamma(beta)\n",
    "    cheby = np.sqrt(np.round(var_prior/(tail_bound)))\n",
    "    x_max = min(99, cheby) # introduced additional bound in case chebyshev is unwieldy\n",
    "    if cheby <= 99:\n",
    "        n_tail = 0\n",
    "        print(\"No Tail\")\n",
    "        \n",
    "    xs = np.linspace(-x_max, x_max, n_samples-2*n_tail)\n",
    "    xs = np.append(-np.logspace(np.log10(cheby), 2, n_tail), xs)\n",
    "    xs = np.append(xs, np.logspace(2, np.log10(cheby), n_tail))\n",
    "    prior_pdf = np.full(xs.shape, np.nan)\n",
    "\n",
    "    # Loop over xs\n",
    "    for j, x in enumerate(xs):\n",
    "\n",
    "        # Define integrands\n",
    "        def gauss_density(theta):\n",
    "            return (1./(np.sqrt(2*np.pi)*theta)) * np.exp(-0.5*(x/theta)**2)\n",
    "\n",
    "        def gen_gamma_density(theta):\n",
    "            return (r/scipy.special.gamma(beta)) * (1/scale) * (theta/scale)**(r*beta - 1) * np.exp(-(theta/scale)**r)\n",
    "\n",
    "        def integrand(theta):\n",
    "            return gauss_density(theta) * gen_gamma_density(theta)\n",
    "\n",
    "        # Integrate \n",
    "        prior_pdf[j] = integrate.quad(integrand, 0, np.inf)[0]\n",
    "\n",
    "    prior_cdf = np.zeros_like(prior_pdf)\n",
    "    for i in range(len(xs) - 1):\n",
    "        prior_cdf[i] = np.trapz(prior_pdf[:i+1], xs[:i+1]) \n",
    "    prior_cdf = np.append(prior_cdf[:-1], 1)\n",
    "\n",
    "    poly = interpolate.CubicSpline(x = xs, y = prior_cdf)\n",
    "    return poly\n",
    "\n",
    "def sample_prior(r, eta, size=1):\n",
    "    '''\n",
    "    Samples from prior distribution of signal x\n",
    "    r : shape parameter, must be > 0\n",
    "    eta : shape parameter, controls roundedness of peak\n",
    "    size : integer specifying number of samples required\n",
    "    '''\n",
    "    vars = gengamma.rvs(a = (eta + 1.5)/r, c = r, size = size)\n",
    "    x = np.random.normal(scale = vars, size=size)\n",
    "    return x\n",
    "\n",
    "def sample_laplace(size=1):\n",
    "    '''\n",
    "    Samples from laplace distribution\n",
    "    size : integer specifying number of samples required\n",
    "    '''\n",
    "    return laplace.rvs(size = size)\n",
    "\n",
    "def cartesian_product(*arrays):\n",
    "    \" Credits: https://stackoverflow.com/a/11146645\"\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)\n",
    "\n",
    "def round_to_2_sigfigs(x):\n",
    "    if x == np.zeros_like(x):\n",
    "        return 0\n",
    "    return np.round(x, -int(np.floor(np.log10(abs(x)))-1))\n",
    "\n",
    "\n",
    "def compute_prior_cdf(r, eta, n_samples = 1000, tail_bound = 0.05, n_tail = 5, scale = 1, scipy_int=True):\n",
    "\n",
    "    '''\n",
    "    Returns PPoly-type function that approximates the prior CDF of the signal x\n",
    "    r : shape parameter controlling rate of exponentional decay\n",
    "    eta : controls roundedness of peak, and hence sparsity\n",
    "    scale : scale parameter\n",
    "    n_samples : number of points used to numerically approximate CDF\n",
    "    tail_bound : Uses Chebyshev's Inequality to bound the region of the CDF that is outside the coverage of xs\n",
    "    n_tail : Sets the number of points tha lie outside the coverage of xs to approximate tails if need be\n",
    "\n",
    "    Usage:\n",
    "    new_cdf = compute_prior_cdf(r = 0.1, eta = 0.001)\n",
    "    new_cdf(0.5343) returns CDF\n",
    "    Can also accept arrays\n",
    "    '''\n",
    "    \n",
    "    beta = (eta + 1.5)/r \n",
    "    var_prior = scale * scipy.special.gamma((eta + 1.5 + 2)/r)/scipy.special.gamma(beta)\n",
    "    cheby = np.sqrt(np.round(var_prior/(tail_bound)))\n",
    "    \n",
    "    x_max = min(99, cheby) # introduced additional bound in case chebyshev is unwieldy\n",
    "    if cheby < 120:\n",
    "        n_tail = 0\n",
    "        print(\"No Tail\")\n",
    "    \n",
    "\n",
    "    xs = np.linspace(-x_max, x_max, n_samples-2*n_tail)\n",
    "    xs = np.append(-np.logspace(np.log10(cheby), 2, n_tail), xs)\n",
    "    xs = np.append(xs, np.logspace(2, np.log10(cheby), n_tail))\n",
    "    prior_pdf = np.full(xs.shape, np.nan)\n",
    "\n",
    "    # Loop over xs\n",
    "    for j, x in enumerate(xs):\n",
    "\n",
    "        # Define integrands\n",
    "        def gauss_density(theta):\n",
    "            return (1./(np.sqrt(2*np.pi)*theta)) * np.exp(-0.5*(x/theta)**2)\n",
    "\n",
    "        def gen_gamma_density(theta):\n",
    "            return (r/scipy.special.gamma(beta)) * (1/scale) * (theta/scale)**(r*beta - 1) * np.exp(-(theta/scale)**r)\n",
    "\n",
    "        def integrand(theta):\n",
    "            return gauss_density(theta) * gen_gamma_density(theta)\n",
    "\n",
    "        # Integrate \n",
    "        if scipy_int:\n",
    "            prior_pdf[j] = integrate.quad(integrand, 0, np.inf)[0]\n",
    "        else:\n",
    "            prior_pdf[j] = eng.testIntegrals(float(r), float(eta), float(x), nargout=1)\n",
    "\n",
    "    prior_cdf = np.zeros_like(prior_pdf)\n",
    "    for i in range(len(xs) - 1):\n",
    "        prior_cdf[i] = np.trapz(prior_pdf[:i+1], xs[:i+1])\n",
    "    prior_cdf = np.append(prior_cdf[:-1], 1)\n",
    "    poly = interpolate.CubicSpline(x = xs, y = prior_cdf)\n",
    "\n",
    "    return poly\n",
    "\n",
    "\n",
    "def round_to_2_sigfigs(x):\n",
    "    if x == np.zeros_like(x):\n",
    "        return 0\n",
    "    return np.round(x, -int(np.floor(np.log10(abs(x)))-1))\n",
    "\n",
    "def add_cdfs(folder_name, r_range, eta_range, n_samples = 10000, scipy_int=True):\n",
    "    '''\n",
    "    folder_name: Name of directory that contains pickles of dictionaries of cdfs\n",
    "    r_range: range of r values, assumes use of np.arange\n",
    "    eta_range: range of eta values, assumes use of np.arange\n",
    "    check_redundant: if True, checks if key already exists in dictionary\n",
    "    n_samples: number of samples used when computing prior_cdf\n",
    "    '''\n",
    "    FOLDER_PATH = f'CDFs\\\\{folder_name}_{n_samples}_{min(eta_range)}-{max(eta_range)}\\\\'\n",
    "    cdfs_completed = set()\n",
    "    if os.path.isdir(FOLDER_PATH):\n",
    "        \n",
    "        for pkl in os.listdir(FOLDER_PATH):\n",
    "            with open(f'{FOLDER_PATH}{pkl}', 'rb') as handle:\n",
    "                next_cdf = pickle.load(handle)\n",
    "            cdfs_completed.update(next_cdf.keys())\n",
    "    else:\n",
    "        Path(os.path.join(os.getcwd(), FOLDER_PATH)).mkdir()\n",
    "\n",
    "    n = len(r_range)*len(eta_range)\n",
    "    i = 0\n",
    "    for r in r_range:\n",
    "        r_cdf = dict()\n",
    "        for eta in eta_range:\n",
    "            (r, eta) = (round_to_2_sigfigs(r), round_to_2_sigfigs(eta))\n",
    "            if ((r, eta) in cdfs_completed):\n",
    "                continue\n",
    "            print(f'{(r, eta)}, {i} of {n}')\n",
    "            i += 1\n",
    "            r_cdf[(r, eta)] = compute_prior_cdf(r = r, eta = eta, n_samples = n_samples,  n_tail = 100, tail_bound = 0.01, scipy_int=scipy_int)\n",
    "\n",
    "        # Store pickle every outer loop iteration as its own file\n",
    "        with open(f'{FOLDER_PATH}/{r}.pickle', 'wb') as handle:\n",
    "            pickle.dump(r_cdf, handle)\n",
    "\n",
    "def extract_single_dist(all_dist_df, base_r, base_eta):\n",
    "    return all_dist_df[(all_dist_df['base_r'] == base_r) & (all_dist_df['base_eta'] == base_eta)].drop(['base_r', 'base_eta'], axis = 1)\n",
    "\n",
    "def kstest_custom(x, cdf, return_loc = False):\n",
    "    n = len(x)\n",
    "    x = np.sort(x)\n",
    "    cdfvals = cdf(x)\n",
    "    dplus, dminus = (np.arange(1.0, n + 1) / n - cdfvals), (cdfvals - np.arange(0.0, n)/n)\n",
    "    plus_amax, minus_amax = dplus.argmax(), dminus.argmax()\n",
    "    loc_max, loc_min = x[plus_amax], x[minus_amax]\n",
    "    d = max(dplus[plus_amax], dminus[minus_amax])\n",
    "    if return_loc:\n",
    "        if d == plus_amax:\n",
    "            return loc_max\n",
    "        else:\n",
    "            return loc_min\n",
    "    return d, kstwo.sf(d, n)\n",
    "\n",
    "def create_obs_x(data_dict, layer, only_diag = False):\n",
    "    df = data_dict[layer]\n",
    "    if not only_diag:\n",
    "        df = df[(df['Orientation'] == 'H') | (df['Orientation'] == 'V')]\n",
    "    else:\n",
    "        df = df[df['Orientation'] == 'D']\n",
    "    return np.hstack(df['Data'])\n",
    "\n",
    "def make_layer_df(x, cdfs_df):\n",
    "    df = cdfs_df.copy()\n",
    "    kstest_stat_pval = cdfs_df.apply(lambda row : kstest_custom(x, row.iloc[3]), axis = 1)\n",
    "    df['kstest_stat'] = kstest_stat_pval.str[0]\n",
    "    df['kstest_pval'] = kstest_stat_pval.str[1] \n",
    "\n",
    "    return df\n",
    "\n",
    "def find_best_metric(obs_x, df, metric='kstest_stat'):\n",
    "    df = df.copy()\n",
    "    kstest_stat_pval = df.apply(lambda row : kstest_custom(obs_x, row.iloc[3]), axis = 1)\n",
    "    df['kstest_stat'] = kstest_stat_pval.str[0]\n",
    "    df['kstest_pval'] = kstest_stat_pval.str[1]\n",
    "    df_metric = df.set_index(['r', 'eta'])[[metric]]\n",
    "    if metric == 'kstest_stat':\n",
    "        idx = df_metric.idxmin()\n",
    "    elif metric == 'kstest_pval':\n",
    "        idx = df_metric.idxmax()\n",
    "    else:\n",
    "        df_metric[metric] = df.apply(metric)\n",
    "        idx = df_metric[[metric]].idxmin()\n",
    "    r = idx.iloc[0][0]\n",
    "    eta = idx.iloc[0][1]\n",
    "    stat = df.set_index(['r', 'eta']).loc[idx, 'kstest_stat'].iloc[0]\n",
    "    pval = df.set_index(['r', 'eta']).loc[idx, 'kstest_pval'].iloc[0]\n",
    "    return np.array([r, eta, stat, pval])\n",
    "\n",
    "def val_df_fixed_num(x, num_samples, df):\n",
    "    val_df = pd.DataFrame(columns = ['r', 'eta', 'kstest_stat', 'kstest_pval'])\n",
    "    all_subsets = np.array_split(x, x.size/num_samples)\n",
    "    for i, x_s in enumerate(all_subsets):\n",
    "        val_df.loc[i, :] = find_best_metric(x_s, df)\n",
    "    val_df['num_samples'] = num_samples * np.ones(val_df.shape[0])\n",
    "    return val_df\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
